{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6db2da9d",
   "metadata": {},
   "source": [
    "### Test data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a870f8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\STACKER BEE\\\\Image classification using CNN'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94a219e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data created successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "# main_data_dir = r'E:\\STACKER BEE\\Image classification using CNN\\image zip file\\tgv3zb82nd-1'\n",
    "# samples_data_dir = r'E:\\STACKER BEE\\Image classification using CNN\\Sample Data'\n",
    "# test_data_dir = r'E:\\STACKER BEE\\Image classification using CNN\\Test Data'\n",
    "main_data_dir = 'E:\\\\STACKER BEE\\\\Image classification using CNN\\\\image zip file\\\\tgv3zb82nd-1'\n",
    "samples_data_dir = 'E:\\\\STACKER BEE\\\\Image classification using CNN\\\\Sample Data'\n",
    "test_data_dir = 'E:\\\\STACKER BEE\\\\Image classification using CNN\\\\test_data'\n",
    "\n",
    "# Create test data directories\n",
    "test_dir_a = os.path.join(test_data_dir, r'E:\\STACKER BEE\\Image classification using CNN\\Test Data')\n",
    "test_dir_b = os.path.join(test_data_dir, r'E:\\STACKER BEE\\Image classification using CNN\\Test Data')\n",
    "os.makedirs(test_dir_a, exist_ok=True)\n",
    "os.makedirs(test_dir_b, exist_ok=True)\n",
    "\n",
    "# Function to select files for testing\n",
    "def select_test_files(main_dir, samples_dir, test_dir):\n",
    "    main_class_dirs = os.listdir(main_dir)\n",
    "    for class_dir in main_class_dirs:\n",
    "        main_class_path = os.path.join(main_dir, class_dir)\n",
    "        samples_class_path = os.path.join(samples_dir, class_dir)\n",
    "        test_class_path = os.path.join(test_dir, class_dir)\n",
    "        os.makedirs(test_class_path, exist_ok=True)\n",
    "\n",
    "        main_files = os.listdir(main_class_path)\n",
    "        samples_files = os.listdir(samples_class_path)\n",
    "\n",
    "        test_files = list(set(main_files) - set(samples_files))\n",
    "\n",
    "        for file in test_files:\n",
    "            src_path = os.path.join(main_class_path, file)\n",
    "            dest_path = os.path.join(test_class_path, file)\n",
    "            shutil.copy(src_path, dest_path)\n",
    "\n",
    "# Select files for testing for classes a and b\n",
    "select_test_files(os.path.join(main_data_dir, r'E:\\STACKER BEE\\Image classification using CNN\\image zip file\\tgv3zb82nd-1\\Healthy-20210326T083815Z-001'), os.path.join(samples_data_dir, r'E:\\STACKER BEE\\Image classification using CNN\\Sample Data'), test_dir_a)\n",
    "select_test_files(os.path.join(main_data_dir,r'E:\\STACKER BEE\\Image classification using CNN\\image zip file\\tgv3zb82nd-1\\Miner-20210326T082341Z-001' ), os.path.join(samples_data_dir, r'E:\\STACKER BEE\\Image classification using CNN\\Sample Data'), test_dir_b)\n",
    "\n",
    "print(\"Test data created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b8fbb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data created successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "main_data_dir = 'E:\\\\STACKER BEE\\\\Image classification using CNN\\\\image zip file\\\\tgv3zb82nd-1'\n",
    "samples_data_dir = 'E:\\\\STACKER BEE\\\\Image classification using CNN\\\\Sample Data'\n",
    "test_data_dir = 'E:\\\\STACKER BEE\\\\Image classification using CNN\\\\test_data'\n",
    "\n",
    "# Create test data directories\n",
    "test_dir_a = os.path.join(test_data_dir, r'E:\\STACKER BEE\\Image classification using CNN\\Test Data')\n",
    "test_dir_b = os.path.join(test_data_dir, r'E:\\STACKER BEE\\Image classification using CNN\\Test Data')\n",
    "os.makedirs(test_dir_a, exist_ok=True)\n",
    "os.makedirs(test_dir_b, exist_ok=True)\n",
    "\n",
    "# Function to select files for testing\n",
    "def select_test_files(main_dir, samples_dir, test_dir, num_files):\n",
    "    main_class_dirs = os.listdir(main_dir)\n",
    "    for class_dir in main_class_dirs:\n",
    "        main_class_path = os.path.join(main_dir, class_dir)\n",
    "        samples_class_path = os.path.join(samples_dir, class_dir)\n",
    "        test_class_path = os.path.join(test_dir, class_dir)\n",
    "        os.makedirs(test_class_path, exist_ok=True)\n",
    "\n",
    "        main_files = os.listdir(main_class_path)\n",
    "        samples_files = os.listdir(samples_class_path)\n",
    "\n",
    "        # Select files that are in main_files but not in samples_files\n",
    "        test_files = [file for file in main_files if file not in samples_files]\n",
    "\n",
    "        # Randomly select num_files from test_files\n",
    "        test_files = random.sample(test_files, min(num_files, len(test_files)))\n",
    "\n",
    "        for file in test_files:\n",
    "            src_path = os.path.join(main_class_path, file)\n",
    "            dest_path = os.path.join(test_class_path, file)\n",
    "            shutil.copy(src_path, dest_path)\n",
    "\n",
    "# Number of files to select for testing from each class\n",
    "num_files = 300\n",
    "\n",
    "# Select files for testing for classes a and b\n",
    "select_test_files(os.path.join(main_data_dir, 'Healthy-20210326T083815Z-001'), samples_data_dir, test_dir_a, num_files)\n",
    "select_test_files(os.path.join(main_data_dir, 'Miner-20210326T082341Z-001'), samples_data_dir, test_dir_b, num_files)\n",
    "\n",
    "print(\"Test data created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0df1198",
   "metadata": {},
   "source": [
    "### Model testing on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b516be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filed saved successfully\n",
      "filed saved successfully\n"
     ]
    }
   ],
   "source": [
    "##Extracting the files and saving the names for testing the saved model\n",
    "import os\n",
    "# Define the path to the 'Healthy' folder within the main_data_dir\n",
    "healthy_folder_path = os.path.join(main_data_dir, r'E:\\STACKER BEE\\Image classification using CNN\\Test Data\\Healthy')\n",
    "miner_folder_path= os.path.join(main_data_dir, r'E:\\STACKER BEE\\Image classification using CNN\\Test Data\\Miner')\n",
    "def data_path(folder_path):\n",
    "    # Get a list of files in the 'Healthy' folder\n",
    "    healthy_files = os.listdir(healthy_folder_path)\n",
    "    files=[]\n",
    "    #Iterate through the files\n",
    "    for file_name in healthy_files:\n",
    "        file_path = os.path.join(healthy_folder_path, file_name)\n",
    "        files.append(file_path)\n",
    "    print('filed saved successfully')\n",
    "    return files\n",
    "healthy_files=data_path(healthy_folder_path)\n",
    "miner_files=data_path(miner_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcce6d2",
   "metadata": {},
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded5f6db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.losses import BinaryCrossentropy\n",
    "#set your model path\n",
    "# Path to the directory containing the SavedModel\n",
    "saved_model_dir = 'E:/STACKER BEE/Image classification using CNN/model'\n",
    "\n",
    "# Load the SavedModel\n",
    "loaded_model = tf.saved_model.load(saved_model_dir)\n",
    "\n",
    "# You can then access the specific model(s) from the loaded SavedModel\n",
    "model = loaded_model.signatures[\"serving_default\"]\n",
    "\n",
    "# Placeholder input data for inference (replace this with your actual input data)\n",
    "input_tensor = tf.constant([[1.0, 2.0, 3.0, 4.0]])\n",
    "\n",
    "# Perform inference using the model\n",
    "result = model(input_tensor)\n",
    "\n",
    "# Or you can convert the model to a Keras model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad204d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\aksha\\AppData\\Local\\Temp\\ipykernel_26232\\3673822284.py:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  saved_model_dir = \"E:\\STACKER BEE\\Image classification using CNN\\model.h5\"\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "saved_model_dir = \"E:\\STACKER BEE\\Image classification using CNN\\model.h5\"\n",
    "keras_model = tf.keras.models.load_model(saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27ba5540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.16.1\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: e:\\progamfiles\\anaconda\\lib\\site-packages\n",
      "Requires: tensorflow-intel\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bdbf813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in e:\\progamfiles\\anaconda\\lib\\site-packages (2.15.0)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.16.1-cp39-cp39-win_amd64.whl (2.1 kB)\n",
      "Collecting tensorflow-intel==2.16.1\n",
      "  Downloading tensorflow_intel-2.16.1-cp39-cp39-win_amd64.whl (376.9 MB)\n",
      "     ------------------------------------ 376.9/376.9 MB 351.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in e:\\progamfiles\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.2)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in e:\\progamfiles\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in e:\\progamfiles\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in e:\\progamfiles\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in e:\\progamfiles\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.10.0)\n",
      "Collecting ml-dtypes~=0.3.1\n",
      "  Downloading ml_dtypes-0.3.2-cp39-cp39-win_amd64.whl (127 kB)\n",
      "     -------------------------------------- 127.7/127.7 kB 7.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: termcolor>=1.1.0 in e:\\progamfiles\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Collecting tensorboard<2.17,>=2.16\n",
      "  Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "Requirement already satisfied: libclang>=13.0.0 in e:\\progamfiles\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in e:\\progamfiles\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in e:\\progamfiles\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.60.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in e:\\progamfiles\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: six>=1.12.0 in e:\\progamfiles\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: setuptools in e:\\progamfiles\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (65.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in e:\\progamfiles\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in e:\\progamfiles\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in e:\\progamfiles\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in e:\\progamfiles\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Collecting keras>=3.0.0\n",
      "  Using cached keras-3.1.1-py3-none-any.whl (1.1 MB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in e:\\progamfiles\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in e:\\progamfiles\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in e:\\progamfiles\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in e:\\progamfiles\\anaconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.38.4)\n",
      "Collecting optree\n",
      "  Downloading optree-0.11.0-cp39-cp39-win_amd64.whl (240 kB)\n",
      "     -------------------------------------- 240.0/240.0 kB 5.0 MB/s eta 0:00:00\n",
      "Collecting namex\n",
      "  Using cached namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
      "Collecting rich\n",
      "  Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\progamfiles\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in e:\\progamfiles\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in e:\\progamfiles\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\progamfiles\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in e:\\progamfiles\\anaconda\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in e:\\progamfiles\\anaconda\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in e:\\progamfiles\\anaconda\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in e:\\progamfiles\\anaconda\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (4.11.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in e:\\progamfiles\\anaconda\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.1)\n",
      "Collecting pygments<3.0.0,>=2.13.0\n",
      "  Using cached pygments-2.17.2-py3-none-any.whl (1.2 MB)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in e:\\progamfiles\\anaconda\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, pygments, optree, ml-dtypes, mdurl, markdown-it-py, tensorboard, rich, keras, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.11.2\n",
      "    Uninstalling Pygments-2.11.2:\n",
      "      Successfully uninstalled Pygments-2.11.2\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.2.0\n",
      "    Uninstalling ml-dtypes-0.2.0:\n",
      "      Successfully uninstalled ml-dtypes-0.2.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.15.1\n",
      "    Uninstalling tensorboard-2.15.1:\n",
      "      Successfully uninstalled tensorboard-2.15.1\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.15.0\n",
      "    Uninstalling keras-2.15.0:\n",
      "      Successfully uninstalled keras-2.15.0\n",
      "  Attempting uninstall: tensorflow-intel\n",
      "    Found existing installation: tensorflow-intel 2.15.0\n",
      "    Uninstalling tensorflow-intel-2.15.0:\n",
      "      Successfully uninstalled tensorflow-intel-2.15.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.15.0\n",
      "    Uninstalling tensorflow-2.15.0:\n",
      "      Successfully uninstalled tensorflow-2.15.0\n",
      "Successfully installed keras-3.1.1 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.3.2 namex-0.0.7 optree-0.11.0 pygments-2.17.2 rich-13.7.1 tensorboard-2.16.2 tensorflow-2.16.1 tensorflow-intel-2.16.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c712a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNN",
   "language": "python",
   "name": "cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
